{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 使用nn.transformer和torchtext搭建语言模型\n",
    "根据论文Attention is all your need，pytorch包含一个标准的transformer模块。比起RNN来说，transfromer在许多序列到序列的任务中具有更高的质量，同时具有更强的并行性。  \n",
    "\n",
    "nn.Transformer模块完全依赖于一种注意力机制（nn.MulitiheadAttention）来控制输入和输出之间的全局依赖关系。  \n",
    "\n",
    "Transformer模块是高度模块化的，因此单个组件(例如nn.TransformerEncoder)可以很容易地适配/组合。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义模型\n",
    "本次的语言模型任务是在一个单词序列出现后为给定单词（或单词序列）赋值一个概率。  \n",
    "\n",
    "如框架图，一系列tokens首先传入embedding层，再进行positional encoding，以确定单词的顺序。  \n",
    "\n",
    "nn.transformerEncoder由nn.TransformerEncoderLayer中的多层组成。  \n",
    "在输入序列的同时，由于nn.transformerEncoder的自注意力层只允许出现在序列中较早的位置，因此需要一个正方形的attention mask。  \n",
    "\n",
    "对于语言建模任务来说，任何在未来可能出现位置上的tokens都应该被mask。  \n",
    "为了在输出单词上生成一个概率分布，nn.transformerEncoder模型的输出通过一个线性层后进行softmax操作。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\n",
    "from typing import Tuple  # typing模块\n",
    "\n",
    "import torch\n",
    "# torch.nn和torch.nn.functional两个库都可以实现神经网络的各层运算。\n",
    "# 其他包括卷积、池化、padding、激活(非线性层)、线性层、正则化层、其他损失函数Loss，两者都可以实现\n",
    "# nn.functional毕竟只是nn的子库，nn的功能要多一些\n",
    "from torch import nn, Tensor  \n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken:int, d_model:int, nhead:int, d_hid:int, \n",
    "                nlayers:int, dropout:float=0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()  # 参数初始化\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encode.weight.data.uniform_(-initrange,initrange)  # 权重随机生成一个实数范围在-0.1，0，1之间，_符号代表直接覆盖类型的函数\n",
    "        self.decoder.bias.data.zero_()  # 偏置设为0\n",
    "        self.decoder.weight.data.uniform_(-initrange,initrange)\n",
    "    \n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            src:张量，[seq_len, batch_size]\n",
    "            src_mask:张量，[seq_len, seq_len] 正方形mask\n",
    "        返回：\n",
    "            张量，形状，[seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)  # 位置编码\n",
    "        output = self.transformer_encoder(src, src_mask)  # 个人感觉核心步骤都封装在在transformer_encoder和decoder里面\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(sz:int) -> Tensor:\n",
    "        # 生成一个上三角矩阵，值为-inf（无穷小）\n",
    "        return torch.triu(torch.ones(sz, sz) * float('inf'), diagonal = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 位置编码模块注入一些关于序列中tokens的相对或绝对位置的信息。\n",
    "## 位置编码与embedding具有相同的尺寸，以便两者可以求和。\n",
    "## 这里，我们使用不同频率的正弦和余弦函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, dropout:float=0.1, max_len:int=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # 生成纵向位置tensor\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # torch.arange(0, d_model, 2)得到一个step为2的等差数列，-math.log(10000.0)是固定值-9.\n",
    "        pe = torch.zeros(max_len, 1, d_model)  # pe 3个维度的0张量\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)  # \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)  # \n",
    "        self.register_buffer('pe', pe)  # 向模块添加缓冲区\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            x:张量，[seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]  # x的0维度所有数据\n",
    "        return self.dropout(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('Py38': conda)"
  },
  "interpreter": {
   "hash": "62792cbd3b5bf2807c49140b9dd77f3574e6f3863eef786f6f586b1ba9747cfd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}