{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor\n",
    "张量是一种特殊的数据结构，与数组和矩阵非常相似。\n",
    "在PyTorch中，我们使用张量来编码模型的输入和输出，以及模型的参数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 导包\n",
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 初始化张量"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 张量可以直接从数据中创建。数据类型是自动推断的。\n",
    "data = [[1,2],[3,4]] #list\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 从numpy数组创建张量\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 从其他张量创建，新张量保留参数张量的属性（形状、数据类型）\n",
    "x_ones = torch.ones_like(x_data) # 保留数据类型\n",
    "x_rand = torch.rand_like(x_data,dtype=torch.float) # 覆盖数据类型\n",
    "x_ones, x_rand"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1, 1],\n",
       "         [1, 1]]),\n",
       " tensor([[0.2201, 0.9070],\n",
       "         [0.5551, 0.7253]]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 指定张量维度\n",
    "# shape 是张量维度的元组。在下面的函数中，它决定了输出张量的维度。\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "rand_tensor"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9823, 0.7189, 0.4722],\n",
       "        [0.9236, 0.4351, 0.4800]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量的属性"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 张量属性描述了它们的形状、数据类型、存储它们的设备\n",
    "tensor = torch.rand(3,4)\n",
    "tensor.shape, tensor.dtype, tensor.device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.float32, device(type='cpu'))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量操作"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 张量操作包括算术，线性代数，矩阵操作(转置，索引，切片)\n",
    "# 默认情况下，张量是在CPU上创建的。\n",
    "# 我们需要使用.to方法显式地将张量移动到GPU(在检查GPU可用性之后)。\n",
    "# 从时间和内存方面来说，在设备之间复制大型张量是非常昂贵的\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# 一些操作上，tensor和numpy数组很类似\n",
    "# 索引和切片\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[0] # 首行\n",
    "tensor[:,0] # 首列\n",
    "tensor[...,-1] # 最后一列"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# 连接张量\n",
    "# 可以使用cat沿给定维度连接一系列张量\n",
    "t1 = torch.cat([tensor,tensor,tensor],dim=1) # dim=1按列连接\n",
    "t1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# 算术运算  \n",
    "# 张量间的矩阵乘法，y1, y2, y3 将具有相同的值\n",
    "# matmul内积\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "y1, y2, y3\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "# 元素乘积\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "z1, z2, z3"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# 单元素张量\n",
    "# 通过张量的所有值聚合为一个常量值，使用item()\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "agg, agg_item, type(agg_item) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(16.), 16.0, float)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# in-place操作\n",
    "# 将结果直接存储到操作数中的操作成为in-place操作，即直接对原视图进行操作改变\n",
    "# 这些操作用_\b后缀表示\n",
    "tensor.add_(5)\n",
    "tensor\n",
    "tensor.copy_(tensor)\n",
    "tensor\n",
    "# in-place操作可以节省一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录\n",
    "# 因此，不鼓励使用它们。"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[21., 21., 21., 21.],\n",
       "        [21., 21., 21., 21.],\n",
       "        [21., 21., 21., 21.],\n",
       "        [21., 21., 21., 21.]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 与numpy的关联"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# CPU 和 NumPy 数组上的张量可以共享它们的底层内存位置，改变一个会改变另一个。\n",
    "# tensor to numpy数组\n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "t, n\n",
    "type(t),type(n)\n",
    "\n",
    "# 改变张量，数组也会变化\n",
    "t.add_(1)\n",
    "t,n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2.]), array([2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# numpy数组 to tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "n, t\n",
    "\n",
    "# NumPy 数组的变化反映在张量中\n",
    "np.add(n, 1, out = n)\n",
    "t, n\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2.], dtype=torch.float64),\n",
       " array([2., 2., 2., 2., 2.]))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('Py38': conda)"
  },
  "interpreter": {
   "hash": "62792cbd3b5bf2807c49140b9dd77f3574e6f3863eef786f6f586b1ba9747cfd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}